#+title: Tapis
#+author: Chahak Mehta
#+date: <2023-01-03 Tue>
#+property: header-args :session tapis :eval no-export :exports both :async yes :tangle no

* =tapis-cli=

#+begin_quote
Tapis CLI is a human-friendly, scriptable command line interface, implemented in Python, that helps scientists and engineers build and manage scalable computational and data science workflow projects using the [[https://tacc-cloud.readthedocs.io/projects/agave/en/latest/][Tapis]] platform.
#+end_quote

#+begin_src shell
tapis --version
#+end_src

#+RESULTS:
: tapis 1.0.6

To initialize a host, run ~tapis auth init~

#+begin_src shell :results raw
tapis auth init
#+end_src

#+RESULTS:
+--------------------+----------------------------------+
| Field              | Value                            |
+--------------------+----------------------------------+
| tenant_id          | designsafe                       |
| username           | chahak                           |
| api_key            | Q***************************     |
| access_token       | f******************************* |
| expires_at         | Sat Jan  7 17:52:20 2023         |
| verify             | True                             |
| registry_url       | https://index.docker.io          |
| registry_username  | chahak                           |
| registry_password  | a*********U                      |
| registry_namespace | chahak                           |
+--------------------+----------------------------------+

To list the files present in my default folder, I simply use the ~tapis files list~ command with the default storage location.
#+begin_src shell :results raw
tapis files list agave://designsafe.storage.default/chahak/
#+end_src

#+RESULTS:
+--------------------+--------------+--------+
| name               | lastModified | length |
+--------------------+--------------+--------+
| test-datadepot.txt | 4 hours ago  |      0 |
+--------------------+--------------+--------+

In this call, the =designsafe.storage.default= is the /systemId/ where the data is stored and /chahak/ is my username (and here, the default path). Both these information can be gleaned from the URL on the browser. For example, if the url is

#+begin_quote
https://www.designsafe-ci.org/data/browser/public/designsafe.storage.community/Jupyter%20Notebooks%20for%20Civil%20Engineering%20Courses/University_of_Washington/freeFieldEffectiveJupyter
#+end_quote

then, ~systemId=designsafe.storage.community~ and ~path=Jupyter%20Notebooks%20for%20Civil%20Engineering%20Courses~. Therefore, the call would begin

#+begin_src shell :results raw
tapis files list agave://designsafe.storage.community/Jupyter%20Notebooks%20for%20Civil%20Engineering%20Courses/University_of_Washington/freeFieldEffectiveJupyter
#+end_src

#+RESULTS:
+---------------------------------------------------------------+--------------+--------+
| name                                                          | lastModified | length |
+---------------------------------------------------------------+--------------+--------+
| ShortReport.rst                                               | 8 months ago |   1380 |
| ShortReport.tex                                               | 8 months ago |   1863 |
| exampleDescription.pptx                                       | 8 months ago |  51605 |
| freeFieldEffective.tcl                                        | 8 months ago |  17925 |
| freeFieldEffectiveJupyter_community.ipynb                     | 8 months ago | 139950 |
| freeFieldEffectiveJupyter_community_2022-07-25 22-40-57.ipynb | 6 months ago | 140107 |
| macros.tex                                                    | 8 months ago |   1954 |
| plotAcc.py                                                    | 8 months ago |   1626 |
| plotPorepressure.py                                           | 8 months ago |    871 |
| plotProfile.py                                                | 8 months ago |   2791 |
| plotStressStrain.py                                           | 8 months ago |    620 |
| respSpectra.py                                                | 8 months ago |   2340 |
| schematic.eps                                                 | 8 months ago | 908760 |
| schematic.png                                                 | 8 months ago |   5246 |
| short.tex                                                     | 8 months ago |   3346 |
| untitled.py                                                   | 8 months ago |      0 |
| velocity.input                                                | 8 months ago | 100260 |
+---------------------------------------------------------------+--------------+--------+

Similarly, if we want the information of a project, say https://www.designsafe-ci.org/data/browser/public/designsafe.storage.published/PRJ-3484,

#+begin_src shell :results raw
tapis files list agave://designsafe.storage.published/PRJ-3484
#+end_src

#+RESULTS:
+--------------------------------------------------+--------------+----------+
| name                                             | lastModified |   length |
+--------------------------------------------------+--------------+----------+
| CDSS_ResultsPlot.m                               | 2 months ago |     3780 |
| Direct Simple Shear Experimental Setup.pdf       | 2 months ago |   333090 |
| Direct Simple Shear Test Sensors.pdf             | 2 months ago |   132480 |
| Summary Report on Ottawa F65 Sand CDSS Tests.pdf | 2 months ago | 11470561 |
| eo_0_576_sigv_40_CSR_0_160_Tau_0_.csv            | 2 months ago |   367421 |
| eo_0_576_sigv_40_CSR_0_170_Tau_0_.csv            | 2 months ago |   266759 |
| eo_0_576_sigv_40_CSR_0_190_Tau_0_.csv            | 2 months ago |   133005 |
| eo_0_576_sigv_40_CSR_0_210_Tau_0_.csv            | 2 months ago |   106531 |
| eo_0_576_sigv_40_CSR_0_240_Tau_0_.csv            | 2 months ago |    73409 |
| eo_0_576_sigv_40_CSR_0_250_Tau_0_.csv            | 2 months ago |    48243 |
| eo_0_576_sigv_40_CSR_0_300_Tau_0_.csv            | 2 months ago |    20694 |
| eo_0_588_sigv_40_CSR_0_150_Tau_0_.csv            | 2 months ago |   426255 |
| eo_0_588_sigv_40_CSR_0_170_Tau_0_.csv            | 2 months ago |   145600 |
| eo_0_588_sigv_40_CSR_0_200_Tau_0_.csv            | 2 months ago |    75184 |
| eo_0_588_sigv_40_CSR_0_230_Tau_0_.csv            | 2 months ago |    57140 |
| eo_0_600_sigv_100_CSR_0_170_Tau_0_.csv           | 2 months ago |    76043 |
| eo_0_600_sigv_100_CSR_0_180_Tau_0_.csv           | 2 months ago |    39150 |
| eo_0_600_sigv_100_CSR_0_190_Tau_0_.csv           | 2 months ago |    24488 |
| eo_0_600_sigv_100_CSR_0_200_Tau_0_.csv           | 2 months ago |    29512 |
| eo_0_600_sigv_100_CSR_0_210_Tau_0_.csv           | 2 months ago |    20205 |
| eo_0_600_sigv_100_CSR_0_220_Tau_0_.csv           | 2 months ago |    19307 |
| eo_0_600_sigv_100_CSR_0_250_Tau_0_.csv           | 2 months ago |    11603 |
| eo_0_601_sigv_100_CSR_0_150_Tau_30_.csv          | 2 months ago |  1350936 |
| eo_0_601_sigv_100_CSR_0_150_Tau_40_.csv          | 2 months ago |   740401 |
| eo_0_601_sigv_100_CSR_0_200_Tau_30_.csv          | 2 months ago |   111957 |
| eo_0_601_sigv_100_CSR_0_200_Tau_40_.csv          | 2 months ago |   194061 |
| eo_0_601_sigv_100_CSR_0_250_Tau_30_.csv          | 2 months ago |    50388 |
| eo_0_601_sigv_100_CSR_0_250_Tau_40_.csv          | 2 months ago |    65815 |
| eo_0_601_sigv_100_CSR_0_300_Tau_30_.csv          | 2 months ago |    22568 |
| eo_0_601_sigv_100_CSR_0_300_Tau_40_.csv          | 2 months ago |    41252 |
| eo_0_601_sigv_40_CSR_0_150_Tau_0_.csv            | 2 months ago |   253281 |
| eo_0_601_sigv_40_CSR_0_170_Tau_0_.csv            | 2 months ago |    95541 |
| eo_0_601_sigv_40_CSR_0_190_Tau_0_.csv            | 2 months ago |    62701 |
| eo_0_601_sigv_40_CSR_0_200_Tau_0_.csv            | 2 months ago |    51772 |
| eo_0_601_sigv_40_CSR_0_200_Tau_10_.csv           | 2 months ago |   956617 |
| eo_0_601_sigv_40_CSR_0_210_Tau_0_.csv            | 2 months ago |    45227 |
| eo_0_601_sigv_40_CSR_0_220_Tau_0_.csv            | 2 months ago |    28760 |
| eo_0_601_sigv_40_CSR_0_220_Tau_10_.csv           | 2 months ago |   428522 |
| eo_0_601_sigv_40_CSR_0_220_Tau_15_.csv           | 2 months ago |  2254872 |
| eo_0_601_sigv_40_CSR_0_230_Tau_10_.csv           | 2 months ago |   351036 |
| eo_0_601_sigv_40_CSR_0_230_Tau_15_.csv           | 2 months ago |  1757060 |
| eo_0_601_sigv_40_CSR_0_240_Tau_10_.csv           | 2 months ago |   282245 |
| eo_0_601_sigv_40_CSR_0_250_Tau_0_.csv            | 2 months ago |    12117 |
| eo_0_601_sigv_40_CSR_0_250_Tau_10_.csv           | 2 months ago |   204689 |
| eo_0_601_sigv_40_CSR_0_250_Tau_15_.csv           | 2 months ago |   661142 |
| eo_0_601_sigv_40_CSR_0_270_Tau_10_.csv           | 2 months ago |   196049 |
| eo_0_601_sigv_40_CSR_0_270_Tau_15_.csv           | 2 months ago |   688310 |
| eo_0_601_sigv_40_CSR_0_300_Tau_10_.csv           | 2 months ago |    40408 |
| eo_0_601_sigv_40_CSR_0_300_Tau_15_.csv           | 2 months ago |   447705 |
| eo_0_601_sigv_40_CSR_0_320_Tau_15_.csv           | 2 months ago |   447356 |
| eo_0_601_sigv_40_CSR_0_350_Tau_15_.csv           | 2 months ago |   128002 |
| eo_0_631_sigv_40_CSR_0_100_Tau_0_.csv            | 2 months ago |   819420 |
| eo_0_631_sigv_40_CSR_0_120_Tau_0_.csv            | 2 months ago |   219298 |
| eo_0_631_sigv_40_CSR_0_140_Tau_0_.csv            | 2 months ago |   138572 |
| eo_0_631_sigv_40_CSR_0_150_Tau_0_.csv            | 2 months ago |    91315 |
| eo_0_631_sigv_40_CSR_0_170_Tau_0_.csv            | 2 months ago |    54473 |
| eo_0_631_sigv_40_CSR_0_180_Tau_0_.csv            | 2 months ago |    58544 |
| eo_0_631_sigv_40_CSR_0_190_Tau_0_.csv            | 2 months ago |    48529 |
+--------------------------------------------------+--------------+----------+

* Setting up a neo4j pod
Ref: https://tapis.readthedocs.io/en/latest/technical/pods.html

- [X] Create TACC Account
  + Username: =chahak=
- [X] Create Docker
  + Username: =chahak=
- [X] Install Tapis Python SDK
  #+begin_src shell
pipenv install tapipy
  #+end_src
- [X] TACC OAuth
  - [X] Create Tapis Client Object
    #+begin_src jupyter-python
import os
from tapipy.tapis import Tapis

t = Tapis(base_url="https://icicle.tapis.io",
          username=os.getenv("TACC_USERNAME"),
          password=os.getenv("TACC_PASSWORD"))

t.get_tokens()
t.access_token.access_token
    #+end_src

    #+RESULTS:
    : eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9...

  - [X] Check Access to the Tapis API
    #+begin_src jupyter-python
t.authenticator.get_profile(username=os.getenv("TACC_USERNAME"))
    #+end_src

    #+RESULTS:
    :
    : create_time: None
    : dn: cn=chahak,ou=People,dc=tacc,dc=utexas,dc=edu
    : email: chahak@utexas.edu
    : given_name: Chahak
    : last_name: Mehta
    : mobile_phone: None
    : phone: None
    : uid: 875122
    : username: chahak

- [X] Registering a templated Pod
  To register a pod, we use the ~pods.create_pod()~ method.
  #+begin_src jupyter-python
t.pods.create_pod(pod_id="tuitustestpod", pod_template="neo4j", description="Creating test pod")
  #+end_src

  #+RESULTS:
  #+begin_example

  command: None
  creation_ts: 2023-01-06T15:34:20.551402
  data_attached: []
  data_requests: []
  description: Creating test pod
  environment_variables:

  networking:
  default:
  port: 7687
  protocol: tcp
  url: tuitustestpod.pods.icicle.tapis.io
  persistent_volume:

  pod_id: tuitustestpod
  pod_template: neo4j
  resources:
  cpu_limit: 2000
  cpu_request: 250
  mem_limit: 3072
  mem_request: 256
  roles_inherited: []
  roles_required: []
  start_instance_ts: None
  status: REQUESTED
  status_container:

  status_requested: ON
  time_to_stop_default: 43200
  time_to_stop_instance: None
  time_to_stop_ts: None
  update_ts: 2023-01-06T15:34:20.551422
  #+end_example


The details and status of this =REQUESTED= pod can be accessed using the ~get_pod()~ method.
#+begin_src jupyter-python
t.pods.get_pod(pod_id="tuitustestpod")
#+end_src

#+RESULTS:
#+begin_example

command: None
creation_ts: 2023-01-06T15:34:20.551402
data_attached: []
data_requests: []
description: Creating test pod
environment_variables:

networking:
default:
port: 7687
protocol: tcp
url: tuitustestpod.pods.icicle.tapis.io
persistent_volume:

pod_id: tuitustestpod
pod_template: neo4j
resources:
cpu_limit: 2000
cpu_request: 250
mem_limit: 3072
mem_request: 256
roles_inherited: []
roles_required: []
start_instance_ts: 2023-01-06T16:06:34.268462
status: RUNNING
status_container:
message: Pod is running.
phase: Running
start_time: 2023-01-06T16:06:17.000000
status_requested: ON
time_to_stop_default: 43200
time_to_stop_instance: None
time_to_stop_ts: 2023-01-07T04:06:34.270380
update_ts: 2023-01-06T15:34:20.551422
#+end_example

The logs can be retrieved using the ~get_pod_logs()~ method.
#+begin_src jupyter-python
t.pods.get_pod_logs(pod_id="tuitustestpod")
#+end_src

#+RESULTS:
#+begin_example

logs: 2023-01-06 16:06:39.132+0000 INFO  Starting...
2023-01-06 16:06:40.347+0000 INFO  This instance is ServerId{e6102ec8} (e6102ec8-c9a0-4650-b0df-b523b7475f5a)
2023-01-06 16:06:42.487+0000 INFO  ======== Neo4j 4.4.16 ========
2023-01-06 16:06:48.846+0000 INFO  [system/00000000] successfully initialized: CREATE USER podsservice IF NOT EXISTS SET PLAINTEXT PASSWORD 'servicepass' SET PASSWORD CHANGE NOT REQUIRED
2023-01-06 16:06:49.145+0000 INFO  [system/00000000] successfully initialized: CREATE USER tuitustestpod IF NOT EXISTS SET PLAINTEXT PASSWORD 'userpass' SET PASSWORD CHANGE NOT REQUIRED
2023-01-06 16:06:56.019+0000 INFO  Upgrading security graph to latest version
2023-01-06 16:06:56.020+0000 INFO  Setting version for 'security-users' to 2
2023-01-06 16:06:56.022+0000 INFO  Upgrading 'security-users' version property from 2 to 3
2023-01-06 16:06:56.283+0000 INFO  Called db.clearQueryCaches(): Query cache already empty.
2023-01-06 16:06:56.439+0000 INFO  Bolt enabled on [0:0:0:0:0:0:0:0%0]:7687.
2023-01-06 16:06:57.632+0000 INFO  Remote interface available at http://pods-tacc-icicle-tuitustestpod:7474/
2023-01-06 16:06:57.637+0000 INFO  id: 8ACF4A544C1C241897F7E92DEED8D4D2266674C622192A7F52378EF69EA3644A
2023-01-06 16:06:57.637+0000 INFO  name: system
2023-01-06 16:06:57.637+0000 INFO  creationDate: 2023-01-06T16:06:43.669Z
2023-01-06 16:06:57.637+0000 INFO  Started.
#+end_example

List of all pods - ~get_pods()~
#+begin_src jupyter-python :results raw drawer
print(t.pods.get_pods())
#+end_src

#+RESULTS:
:results:
#+begin_example
[
command: None
creation_ts: 2023-01-06T15:34:20.551402
data_attached: []
data_requests: []
description: Creating test pod
environment_variables: 

networking: 
default: 
port: 7687
protocol: tcp
url: tuitustestpod.pods.icicle.tapis.io
persistent_volume: 

pod_id: tuitustestpod
pod_template: neo4j
resources: 
cpu_limit: 2000
cpu_request: 250
mem_limit: 3072
mem_request: 256
roles_inherited: []
roles_required: []
start_instance_ts: 2023-01-06T16:06:34.268462
status: RUNNING
status_container: 
message: Pod is running.
phase: Running
start_time: 2023-01-06T16:06:17.000000
status_requested: ON
time_to_stop_default: 43200
time_to_stop_instance: None
time_to_stop_ts: 2023-01-07T04:06:34.270380
update_ts: 2023-01-06T15:34:20.551422]
#+end_example
:end:


Any pod can be deleted using the ~delete_pod(pod_id)~ method.
#+begin_src jupyter-python :results raw
t.pods.delete_pod(pod_id="tuitustestpod")
#+end_src

#+RESULTS:
| message | : | Pod successfully deleted. | metadata | : | nil | result | : |   | status | : | success | version | : | dev |
* Fetching file information using =tapipy=
+ Create ~Tapis~ object and generate a token.
  #+begin_src jupyter-python
import os
from tapipy.tapis import Tapis

t = Tapis(base_url="https://designsafe.tapis.io",
          username=os.getenv("TACC_USERNAME"),
          password=os.getenv("TACC_PASSWORD"))
t.get_tokens()
# t.access_token
  #+end_src

  #+RESULTS:

  Using the ~systemId~ as =designsafe.storage.default=, fetch files.
  #+begin_src jupyter-python
t.files.listFiles(systemId="designsafe.storage.default", path="chahak")
  #+end_src

  #+RESULTS:
  :RESULTS:
  # [goto error]
  #+begin_example
  ---------------------------------------------------------------------------
  NotFoundError                             Traceback (most recent call last)
  Cell In[6], line 1
  ----> 1 t.files.listFiles(systemId="designsafe.storage.default", path="chahak")

  File ~/.local/share/virtualenvs/tuitus-75BIUZJs/lib/python3.9/site-packages/tapipy/tapis.py:1191, in Operation.__call__(self, **kwargs)
     1189     raise errors.ForbiddenError(msg=error_msg, version=version, request=r, response=resp)
     1190 if resp.status_code == 404:
  -> 1191     raise errors.NotFoundError(msg=error_msg, version=version, request=r, response=resp)
     1192 if resp.status_code == 503:
     1193     raise errors.ServiceUnavailableError(msg=error_msg, version=version, request=r, response=resp)

  NotFoundError: message: SYSAPI_NOT_FOUND Record not found. jwtTenant: admin jwtUser: files OboTenant: designsafe OboUser: chahak System: designsafe.storage.default
  #+end_example
  :END:

  Seems like tapipy is using v3 apis which doesn't seem to have any systems that it retrieves via ~getSystems()~ on =designsafe=. Not sure if I can work with this right now.
* Get model configurations for experiments
+ I'm trying to find a way to get information about the model configurations and experiment setup on this page. I found using inspect element that there is an API call to https://www.designsafe-ci.org/api/projects/publication/PRJ-3484/ and that has a field for =modelConfigs= that has experiments but it has =associationIds= which are UUIDs if I'm not wrong, and the above API is the only way I could find to resolve UUIDs but it doesn't seem to be working.
+ According to Dr. Krishna, UUID lookup is not available in designsafe yet so letting this be for now.
* Use ~requests~ module to retrieve files for projects
+ Even without model configs for experiments, next thing to do is to get all the files for all the projects.
** Get a list of projects
Using the ~requests~ module, we can create a request to =https://designsafe-ci.org/api/publications/listing= to get a list of the projects.

#+begin_src jupyter-python
from pprint import pprint
import requests

url = "https://designsafe-ci.org/api/publications/listing"

querystring = {"limit":"3"}

payload = ""
headers = {
    # "cookie": "django_language=en-us",
    # "Host": "www.designsafe-ci.org",
    "User-Agent": "Mozilla/5.0 (X11; Linux x86_64; rv:108.0) Gecko/20100101 Firefox/108.0",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.5",
    "Accept-Encoding": "gzip, deflate, br",
    "DNT": "1",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1",
    "Sec-Fetch-Dest": "document",
    "Sec-Fetch-Mode": "navigate",
    "Sec-Fetch-Site": "none",
    "Sec-Fetch-User": "?1",
    "TE": "trailers"
}

response = requests.request("GET", url, data=payload, headers=headers, params=querystring)

pprint(response.json())
#+end_src

#+RESULTS:
#+begin_example
{'listing': [{'created': '2023-01-09T21:13:07.982682',
              'pi': 'Zhang, Wenyang',
              'project': {'value': {'keywords': 'Physics-based ground motion '
                                                'simulation, loss and '
                                                'resilience assessments, '
                                                'regional analysis, Hercules, '
                                                'SimCenter R2D',
                                    'pi': 'wz_tacc',
                                    'projectType': 'simulation',
                                    'title': 'Regional-scale physics-based '
                                             'ground motion simulation for '
                                             'Istanbul, Turkey'}},
              'projectId': 'PRJ-3712'},
             {'created': '2023-01-09T18:06:55.086860',
              'pi': 'Akin, Idil Deniz',
              'project': {'value': {'keywords': 'suction, water content, '
                                                'post-wildfire',
                                    'pi': 'idil',
                                    'projectType': 'field_recon',
                                    'title': '2019 Williams Flats Wildfire '
                                             'Slope Stability'}},
              'projectId': 'PRJ-3685'},
             {'created': '2023-01-09T01:52:04.001722',
              'pi': 'Matta, Fabio',
              'project': {'value': {'keywords': 'Compressive strength, earth '
                                                'masonry, isotropy, '
                                                'homogeneity, statistical '
                                                'analysis',
                                    'pi': 'fmatta',
                                    'projectType': 'experimental',
                                    'title': 'Physico-Mechanical '
                                             'Characterization of Homogeneity '
                                             'and Isotropy of Prototype Earth '
                                             'Block Material'}},
              'projectId': 'PRJ-2809'}]}
#+end_example


** Get files for a project

For a given =projectId=, we need to get all the result files. The information for projects can be fetched from =https://designsafe-ci.org/api/projects/publication/{projectId}=. For example,

#+begin_src jupyter-python
projectId = "PRJ-3484"
url = f"https://designsafe-ci.org/api/projects/publication/{projectId}"

payload = ""
headers = {"cookie": "django_language=en-us"}

response = requests.request("GET", url, data=payload, headers=headers)
response_json = response.json()
#+end_src

#+RESULTS:

+ The response is a humongous \(\sym\) 1300 lines json object which contains basically all the information about the project that is present on the website. The fields of major importance right now would be ~["eventsList"]~. This corresponds to the different experiment setups.
+ For every element in ~"eventsList"~, ~"fileObjs"~ is the list of files for that experiment, ~["value"]["title"]~ is the title string of the experiment.

#+begin_src jupyter-python
files_info = dict()
for i, event in enumerate(response_json["eventsList"]):
    files_info[i] = {
        "title": event["value"]["title"],
        "files": event["fileObjs"]
        }
pprint(files_info[0])
#+end_src

#+RESULTS:
#+begin_example
{'files': [{'name': 'eo_0_601_sigv_100_CSR_0_150_Tau_40_.csv',
            'path': '/eo_0_601_sigv_100_CSR_0_150_Tau_40_.csv',
            'system': 'project-3122606422211489300-242ac118-0001-012',
            'type': 'file'},
           {'name': 'eo_0_601_sigv_100_CSR_0_200_Tau_40_.csv',
            'path': '/eo_0_601_sigv_100_CSR_0_200_Tau_40_.csv',
            'system': 'project-3122606422211489300-242ac118-0001-012',
            'type': 'file'},
           {'name': 'eo_0_601_sigv_100_CSR_0_250_Tau_40_.csv',
            'path': '/eo_0_601_sigv_100_CSR_0_250_Tau_40_.csv',
            'system': 'project-3122606422211489300-242ac118-0001-012',
            'type': 'file'},
           {'name': 'eo_0_601_sigv_100_CSR_0_300_Tau_40_.csv',
            'path': '/eo_0_601_sigv_100_CSR_0_300_Tau_40_.csv',
            'system': 'project-3122606422211489300-242ac118-0001-012',
            'type': 'file'}],
 'title': 'CDSS tests with Sig_v = 100 kPa, Dr = 67%, Alpha = 0.4'}
#+end_example

Now, to download any of those file, I can either create a request manually or use ~tapis cli~ to download it.

#+begin_src bash
tapis files download agave://designsafe.storage.published/PRJ-3484/eo_0_631_sigv_40_CSR_0_190_Tau_0_.csv
#+end_src

#+RESULTS:

~tapis cli~ doesn't seem to allow specifying an output directory to save the file to, which is definitely not great as I'll have to either move them myself or change directory every time before saving the file. On the other hand, it has some support for multi-threaded downloads. Need more investigation on this.

On the other hand, ~tapis cli~ uses ~agavepy~ to download the files. So, for now, I could potentially use that directly to download these files.

#+begin_src jupyter-python
from agavepy import Agave
ag = Agave.restore()
#+end_src

#+RESULTS:

#+begin_src jupyter-python
rsp = ag.files.download(systemId="designsafe.storage.published", filePath="PRJ-3484/eo_0_631_sigv_40_CSR_0_190_Tau_0_.csv")
if isinstance(rsp, dict):
    raise Error(f"Failed to download")
with open('test.csv', 'wb') as dest_file:
    for block in rsp.iter_content(4096):
        if not block:
            break
        dest_file.write(block)

print(os.path.isfile('test.csv'))
#+end_src

#+RESULTS:
: True

This tests works. So, I can create a function to do this and create a folder structure with files based on the experiment titles.

** Download entire project info together
Now it is time to combine all of the above together to get all the files of a project in a single go.

#+begin_src jupyter-python :tangle ../../src/download_project.py
import os
import requests
from tqdm import tqdm
from agavepy import Agave
ag = Agave.restore()


def download_file(filepath, dirpath):
    rsp = ag.files.download(systemId="designsafe.storage.published",
                            filePath=filepath)
    if isinstance(rsp, dict):
        raise Error(f"Failed to download: {filepath}")
    if not os.path.isdir(dirpath):
        os.makedirs(dirpath)
    with open(os.path.join(dirpath, os.path.basename(filepath)), "wb") as f:
        try:
            for block in rsp.iter_content(4096):
                if not block:
                    break
                f.write(block)
        except Exception as e:
            raise e
    return True


def download_project(projectId):
    url = f"https://designsafe-ci.org/api/projects/publication/{projectId}"

    payload = ""
    headers = ""

    response = requests.get(url, data=payload, headers=headers)
    response_json = response.json()

    files_info = dict()
    for i, event in enumerate(response_json["eventsList"]):
        files_info[i] = {
            "title": event["value"]["title"],
            "files": event["fileObjs"],
        }

    for event, data in tqdm(files_info.items()):
        dirpath = f"{projectId}/{data['title']}"
        for file in tqdm(data["files"], leave=False):
            filepath = f"{projectId}/{file['path']}"
            download_file(filepath, dirpath)

    return


projectId = "PRJ-3484"
download_project(projectId)
#+end_src

This downloads the project files and stores the files in the following structure.

#+begin_src bash :results raw
tree PRJ-3484
#+end_src

#+RESULTS:
#+begin_example
PRJ-3484
├── CDSS tests with Sig_v = 100 kPa, Dr = 66.7%, Alpha = 0
│   ├── eo_0_600_sigv_100_CSR_0_170_Tau_0_.csv
│   ├── eo_0_600_sigv_100_CSR_0_180_Tau_0_.csv
│   ├── eo_0_600_sigv_100_CSR_0_190_Tau_0_.csv
│   ├── eo_0_600_sigv_100_CSR_0_200_Tau_0_.csv
│   ├── eo_0_600_sigv_100_CSR_0_210_Tau_0_.csv
│   ├── eo_0_600_sigv_100_CSR_0_220_Tau_0_.csv
│   └── eo_0_600_sigv_100_CSR_0_250_Tau_0_.csv
├── CDSS tests with Sig_v = 100 kPa, Dr = 67%, Alpha = 0.3
│   ├── eo_0_601_sigv_100_CSR_0_150_Tau_30_.csv
│   ├── eo_0_601_sigv_100_CSR_0_200_Tau_30_.csv
│   ├── eo_0_601_sigv_100_CSR_0_250_Tau_30_.csv
│   └── eo_0_601_sigv_100_CSR_0_300_Tau_30_.csv
├── CDSS tests with Sig_v = 100 kPa, Dr = 67%, Alpha = 0.4
│   ├── eo_0_601_sigv_100_CSR_0_150_Tau_40_.csv
│   ├── eo_0_601_sigv_100_CSR_0_200_Tau_40_.csv
│   ├── eo_0_601_sigv_100_CSR_0_250_Tau_40_.csv
│   └── eo_0_601_sigv_100_CSR_0_300_Tau_40_.csv
├── CDSS tests with Sig_v = 40 kPa, Dr = 55.2%, Alpha = 0
│   ├── eo_0_631_sigv_40_CSR_0_100_Tau_0_.csv
│   ├── eo_0_631_sigv_40_CSR_0_120_Tau_0_.csv
│   ├── eo_0_631_sigv_40_CSR_0_140_Tau_0_.csv
│   ├── eo_0_631_sigv_40_CSR_0_150_Tau_0_.csv
│   ├── eo_0_631_sigv_40_CSR_0_170_Tau_0_.csv
│   ├── eo_0_631_sigv_40_CSR_0_180_Tau_0_.csv
│   └── eo_0_631_sigv_40_CSR_0_190_Tau_0_.csv
├── CDSS tests with Sig_v = 40 kPa, Dr = 66.2%, Alpha = 0
│   ├── eo_0_601_sigv_40_CSR_0_150_Tau_0_.csv
│   ├── eo_0_601_sigv_40_CSR_0_170_Tau_0_.csv
│   ├── eo_0_601_sigv_40_CSR_0_190_Tau_0_.csv
│   ├── eo_0_601_sigv_40_CSR_0_200_Tau_0_.csv
│   ├── eo_0_601_sigv_40_CSR_0_210_Tau_0_.csv
│   ├── eo_0_601_sigv_40_CSR_0_220_Tau_0_.csv
│   └── eo_0_601_sigv_40_CSR_0_250_Tau_0_.csv
├── CDSS tests with Sig_v = 40 kPa, Dr = 66.4%, Alpha = 0.25
│   ├── eo_0_601_sigv_40_CSR_0_200_Tau_10_.csv
│   ├── eo_0_601_sigv_40_CSR_0_220_Tau_10_.csv
│   ├── eo_0_601_sigv_40_CSR_0_230_Tau_10_.csv
│   ├── eo_0_601_sigv_40_CSR_0_240_Tau_10_.csv
│   ├── eo_0_601_sigv_40_CSR_0_250_Tau_10_.csv
│   ├── eo_0_601_sigv_40_CSR_0_270_Tau_10_.csv
│   └── eo_0_601_sigv_40_CSR_0_300_Tau_10_.csv
├── CDSS tests with Sig_v = 40 kPa, Dr = 66.4%, Alpha = 0.375
│   ├── eo_0_601_sigv_40_CSR_0_220_Tau_15_.csv
│   ├── eo_0_601_sigv_40_CSR_0_230_Tau_15_.csv
│   ├── eo_0_601_sigv_40_CSR_0_250_Tau_15_.csv
│   ├── eo_0_601_sigv_40_CSR_0_270_Tau_15_.csv
│   ├── eo_0_601_sigv_40_CSR_0_300_Tau_15_.csv
│   ├── eo_0_601_sigv_40_CSR_0_320_Tau_15_.csv
│   └── eo_0_601_sigv_40_CSR_0_350_Tau_15_.csv
├── CDSS tests with Sig_v = 40 kPa, Dr = 71.2%, Alpha = 0
│   ├── eo_0_588_sigv_40_CSR_0_150_Tau_0_.csv
│   ├── eo_0_588_sigv_40_CSR_0_170_Tau_0_.csv
│   ├── eo_0_588_sigv_40_CSR_0_200_Tau_0_.csv
│   └── eo_0_588_sigv_40_CSR_0_230_Tau_0_.csv
└── CDSS tests with Sig_v = 40 kPa, Dr = 75.5%, Alpha = 0
    ├── eo_0_576_sigv_40_CSR_0_160_Tau_0_.csv
    ├── eo_0_576_sigv_40_CSR_0_170_Tau_0_.csv
    ├── eo_0_576_sigv_40_CSR_0_190_Tau_0_.csv
    ├── eo_0_576_sigv_40_CSR_0_210_Tau_0_.csv
    ├── eo_0_576_sigv_40_CSR_0_240_Tau_0_.csv
    ├── eo_0_576_sigv_40_CSR_0_250_Tau_0_.csv
    └── eo_0_576_sigv_40_CSR_0_300_Tau_0_.csv

10 directories, 54 files
#+end_example
* [DEPRECATED] Generate summary and metadata info for the files
UPDATE: See section below for updated function for summary
Next step in building a semantic search engine would be to generate the metadata info/summary for the files inside any project. This can be done using the =pandas= library.

#+begin_src jupyter-python
import os
import pandas as pd
from pprint import pprint

def summarize_csv(filepath):
    """
    Summarize a CSV

    This function generates a dictionary that contains the summary
    of the CSV. It also performs some preprocessing on the column
    names by stripping off any whitespace before and after the column
    name and converting the column name to a snake-case. Furthermore,
    it also adds an "increment_<column_name>" key to the summary that
    records the increments used in the first column of the CSV.
    """
    extension = os.path.splitext(filepath)[1].lower()
    if extension == ".csv":
        df = pd.read_csv(filepath)
    elif extension == ".txt":
        df = pd.read_csv(filepath, engine="python", sep=None)
    else:
        return None, dict()
    df.columns = df.columns.str.strip().str.replace("\s+", " ", regex=True).str.lower().str.replace(" ", "_")
    summary = df.describe().to_dict()
    increment = df[df.columns[0]].iloc[2] - df[df.columns[0]].iloc[1]
    summary[f"increment_{df.columns[0]}"] = increment
    return df, summary


# df, summary = summarize_csv("PRJ-3484/CDSS tests with Sig_v = 100 kPa, Dr = 66.7%, Alpha = 0/eo_0_600_sigv_100_CSR_0_170_Tau_0_.csv")
df, summary = summarize_csv("PRJ-1783/Cyclic Stress-Controlled Triaxial Tests/eo_0_515_po_100/eo_0_515_po_100_csr_0_3.txt")
pprint(summary.keys())
#+end_src

#+RESULTS:
: dict_keys(['time_min', 'eps_a', 'eps_v', 'sigmap_3_kpa', 'sigma_3_kpa', 'sigma_d_kpa', 'increment_time_min'])
* Store information in a neo4j database
As starting step for creating the graph database, we'll create a structure that would enable the queries of the type

#+begin_quote
What projects record information regarding shear_strain?
#+end_quote

** Connecting to a Neo4j database
I've created a neo4j project locally. I can now use =py2neo= to connect to this database.

#+begin_src jupyter-python
from py2neo import Graph

tuitus_db = Graph("bolt://localhost:7687", user="neo4j", password="tuitusci")
#+end_src

#+RESULTS:
** Adding nodes
To add node for a project, say PRJ-3484, we use the =Graph.create= method with the =Node= class.

#+begin_src jupyter-python
from py2neo import Node, Relationship

project_node = Node("Project", name="PRJ-3484")
tuitus_db.create(project_node)
#+end_src

#+RESULTS:

This project will be supervised by a PI, information for which can be found in the project JSON.

#+begin_src jupyter-python
pi_node = Node("Author", name="mmanzari")
tuitus_db.create(pi_node)
pi_rel = Relationship(pi_node, "SUPERVISES", project_node)
tuitus_db.create(pi_rel)
#+end_src

#+RESULTS:

A project can have multiple experiments, each of which can record different information.

#+begin_src jupyter-python
# len(tuitus_db.nodes.match("Project", name="PRJ-3484"))

project_id = "PRJ-3484"
for root, dirs, files in os.walk(project_id):
    for f in files:
        _, summary =
 summarize_csv(os.path.join(root, f))
        for key in summary.keys():
            matching = tuitus_db.nodes.match("Data", name=key)
            if len(matching) == 0:
                column_node = Node("Data", name=key)
                tuitus_db.create(column_node)
            else:
                column_node = list(matching)[0]
            if len(tuitus_db.relationships.match(nodes=(project_node, column_node), r_type="RECORDS")) == 0:
                col_rel = Relationship(project_node, "RECORDS", column_node)
                tuitus_db.create(col_rel)


#+end_src

#+RESULTS:

I can now combine all these steps into a single function that adds the nodes for a given project with files stored in a directory.

#+begin_src jupyter-python
def create_project_data_graph(project_id, project_path, pi, graph_db):
    project_matching = graph_db.nodes.match("Project", name=project_id)
    if len(project_matching) == 0:
        project_node = Node("Project", name=project_id)
        graph_db.create(project_node)
    else:
        project_node = list(project_matching)[0]
    pi_matching = graph_db.nodes.match("Author", name=pi)
    if len(pi_matching) == 0:
        pi_node = Node("Author", name=pi)
        graph_db.create(pi_node)
    else:
        pi_node = list(pi_matching)[0]
    pi_rel = Relationship(pi_node, "SUPERVISES", project_node)
    graph_db.create(pi_rel)

    for root, dirs, files in os.walk(project_id):
        for f in files:
            _, summary = summarize_csv(os.path.join(root, f))
            for key in summary.keys():
                matching = graph_db.nodes.match("Data", name=key)
                if len(matching) == 0:
                    column_node = Node("Data", name=key)
                    graph_db.create(column_node)
                else:
                    column_node = list(matching)[0]
                if len(graph_db.relationships.match(nodes=(project_node, column_node), r_type="RECORDS")) == 0:
                    col_rel = Relationship(project_node, "RECORDS", column_node)
                    graph_db.create(col_rel)

    return


# create_project_data_graph("PRJ-3484", "./", "mmanzari", tuitus_db)
# create_project_data_graph("PRJ-2137", "./", "mmanzari", tuitus_db)
#+end_src

#+RESULTS:

This generates a graph db that can now answer the query mentioned above.

#+begin_src jupyter-python
tuitus_db.run("MATCH (p:PROJECT)-[:RECORDS]->(d:Data {name: 'cycle'}) RETURN p, d")
#+end_src


#+DOWNLOADED: screenshot @ 2023-01-30 16:18:10
[[file:../assets/2023-01-30_16-18-10_screenshot.png]]
* Extract experiment and model config information from the project JSON

#+begin_src jupyter-python
import requests
from pprint import pprint

projectId = "PRJ-3484"
url = f"https://designsafe-ci.org/api/projects/publication/{projectId}"

payload = ""
headers = {}

response = requests.request("GET", url, data=payload, headers=headers)
response_json = response.json()
#+end_src

#+RESULTS:

+ The response JSON has 3 top-level lists that are of interest - =eventsList=, =modelConfigs=, =experimentsList=. =experimentsList= is a list of all the experiments in the project. Each element of the list contains a =uuid= field that is used to link an experiment with a model config or an event. Furthermore, it also has =["value"]["title"]= which gives the name of the experiment.
+ Now, each element in =eventsList= and =modelConfigs= contains a field =["value"]["experiments"]= which is a list of all experiments that the event or the model config is a part of. All of this can be mapped to a project.
+ Each element in =eventsList= also has a =["value"]["modelConfigs"]= which is a list of uuids of models that an event is a part of.

With this information, I can now update the ~create_project_graph~ function to include everything.
#+begin_src jupyter-python
import os
from py2neo import Graph, Node, Relationship

def _create_or_return_node(node_type, node_key, graphdb, **node_props):
    matching = graphdb.nodes.match(node_type, **node_props)
    if len(matching) == 0:
        node = Node(node_type, **node_props)
        graphdb.create(node)
    else:
        node = list(matching)[0]
    return node

def create_project_graph(project_info, project_path, graphdb):
    project_id = project_info.get("project_id", None)
    if project_id is None:
        project_id = project_info.get("projectId", None)
    if project_id is None:
        raise KeyError("project_id or projectId not found in project info.")

    project_props = {"name": project_id}
    project_node = _create_or_return_node(
        "Project", {"name": project_id}, graphdb, **project_props
    )

    pi = project_info["project"]["value"]["pi"]
    pi_props = {"name": pi}
    pi_node = _create_or_return_node(
        "Author", {"name": pi}, graphdb, **pi_props
    )
    pi_project_rel = Relationship(pi_node, "SUPERVISES", project_node)
    graphdb.create(pi_project_rel)

    for exp in project_info["experimentsList"]:
        experiment_props = {
            "uuid": exp["uuid"],
            "title": exp["value"]["title"],
        }
        exp_node = _create_or_return_node(
            "Experiment", {"uuid": exp["uuid"]}, graphdb, **experiment_props
        )
        exp_project_rel = Relationship(project_node, "HAS_EXPERIMENT", exp_node)
        graphdb.create(exp_project_rel)

    for model in project_info["modelConfigs"]:
        model_props = {
            "uuid": model["uuid"],
            "title": model["value"]["title"],
        }
        model_node = _create_or_return_node(
            "Model", {"uuid": model["uuid"]}, graphdb, **model_props
        )
        model_exps = model["value"]["experiments"]
        for exp_uuid in model_exps:
            exp_matching = graphdb.nodes.match("Experiment", uuid=exp_uuid)
            if len(exp_matching) > 0:
                exp_node = list(exp_matching)[0]
                model_experiment_rel = Relationship(exp_node, "HAS_MODEL", model_node)
                graphdb.create(model_experiment_rel)

    for event in project_info["eventsList"]:
        event_props = {
            "title": event["value"]["title"],
        }
        event_node = _create_or_return_node(

            "Event", {"title": event_props["title"]}, graphdb, **event_props
        )
        event_exps = event["value"]["experiments"]
        for exp_uuid in event_exps:
            exp_matching = graphdb.nodes.match("Experiment", uuid=exp_uuid)
            if len(exp_matching) > 0:
                exp_node = list(exp_matching)[0]
                event_experiment_rel = Relationship(exp_node, "HAS_EVENT", event_node)
                graphdb.create(event_experiment_rel)

        event_models = event["value"]["modelConfigs"]
        for model_uuid in event_models:
            model_matching = graphdb.nodes.match("Model", uuid=model_uuid)
            if len(model_matching) > 0:
                model_node = list(model_matching)[0]
                event_model_rel = Relationship(model_node, "HAS_EVENT", event_node)
                graphdb.create(event_model_rel)

        for root, dirs, files in os.walk(
                os.path.join(project_path, project_id, event_props["title"])
        ):
            for f in files:
                filepath = os.path.join(root, f)
                file_node = _create_or_return_node("File", {"filepath": filepath}, graphdb, **{"filepath": filepath})
                if len(graphdb.relationships.match(nodes=(event_node, file_node), r_type="HAS_FILE")) == 0:
                    event_file_rel = Relationship(event_node, "HAS_FILE", file_node)
                    graphdb.create(event_file_rel)
                _, summary = summarize_csv(filepath)
                for key, props in summary.items():
                    if not isinstance(props, dict):
                        props = {key: props}
                    col_props = {"name": key, **props}
                    column_node = _create_or_return_node(
                        "Data", {"name": key}, graphdb, **col_props
                    )
                    if len(graphdb.relationships.match(nodes=(project_node, column_node), r_type="RECORDS")) == 0:
                        col_proj_rel = Relationship(project_node, "RECORDS", column_node)
                        graphdb.create(col_proj_rel)
                    if len(graphdb.relationships.match(nodes=(event_node, column_node), r_type="RECORDS")) == 0:
                        col_event_rel = Relationship(event_node, "RECORDS", column_node)
                        graphdb.create(col_event_rel)
                    if len(graphdb.relationships.match(nodes=(file_node, column_node), r_type="RECORDS")) == 0:
                        col_file_rel = Relationship(file_node, "RECORDS", column_node)
                        graphdb.create(col_file_rel)
    return

create_project_graph(response_json, "./", tuitus_db)
#+end_src

#+RESULTS:

With this, we now have a much dense graph with relationships that can be queried.

#+DOWNLOADED: screenshot @ 2023-02-01 13:26:07
#+attr_html: :width 60%
[[file:../assets/Extract_experiment_and_model_config_information_from_the_project_JSON/2023-02-01_13-26-07_screenshot.png]]

Example query:

#+begin_quote
List all the projects, experiments, models and events that record "axial_stress".
#+end_quote

#+DOWNLOADED: screenshot @ 2023-02-01 13:34:40
#+attr_html: :width 60%
[[file:../assets/Extract_experiment_and_model_config_information_from_the_project_JSON/2023-02-01_13-34-40_screenshot.png]]
* Update ~summarize_csv~ to include categorical variables and round decimals
The current ~summarize_csv~ doesn't include categorical columns in the summary. This means that if there were a column without numeric values, it would not be present in the summary. Columns can be included by passing the ~include="all"~ parameter to the ~.describe()~ function. Though, this introduces another issue as now there are multiple NaN values in the summary depending on the type of column. This information is unnecessary for the database node properties and should be removed. It also rounds off all the summary statistics to upto 5 decimal places so that the values are tractable in the database search. The new function takes care of all of this.

#+begin_src jupyter-python
import os
import pandas as pd
from pprint import pprint

def summarize_csv(filepath):
    """
    Summarize a CSV

    This function generates a dictionary that contains the summary
    of the CSV. It also performs some preprocessing on the column
    names by stripping off any whitespace before and after the column
    name and converting the column name to a snake-case. Furthermore,
    it also adds an "increment_<column_name>" key to the summary that
    records the increments used in the first column of the CSV.
    """
    extension = os.path.splitext(filepath)[1].lower()
    if extension == ".csv":
        df = pd.read_csv(filepath)
    elif extension == ".txt":
        df = pd.read_csv(filepath, engine="python", sep=None)
    else:
        return None, dict()
    df.columns = df.columns.str.strip().str.replace("\s+", "", regex=True).str.lower()
    summary = df.describe(include="all").round(2).to_dict()
    summary = {
        colname: colsummary for colname, colsummary in summary.items() for k, v in colsummary.items() if not pd.isna(v)
    }
    for colname, colsummary in summary.items():
        for k, v in colsummary.items():
            try:
                colsummary[k] = v.item()
            except AttributeError:
                pass
    increment = (df[df.columns[0]].iloc[2] - df[df.columns[0]].iloc[1]).round(5).item()
    summary[f"increment_{df.columns[0]}"] = increment
    return df, summary


# df, summary = summarize_csv("PRJ-3484/CDSS tests with Sig_v = 100 kPa, Dr = 66.7%, Alpha = 0/eo_0_600_sigv_100_CSR_0_170_Tau_0_.csv")
df, summary = summarize_csv("PRJ-1783/Cyclic Stress-Controlled Triaxial Tests/eo_0_515_po_100/eo_0_515_po_100_csr_0_3.txt")
#+end_src

#+RESULTS:

* Querying database using =py2neo=
With the new properties related to the summaries in the nodes, we can now query them to get information filtered on the values from the summary. For example, there are a total of 3 =Data= nodes in the graph with means 20.5, 30.5, 38.06454 respectively. We can now query for all the Nodes which have a mean higher than 30.

#+begin_src jupyter-python
tuitus_db.run("MATCH (d:Data {name: 'normal_stress'}) RETURN d")
#+end_src

#+begin_src jupyter-python
tuitus_db.run("MATCH (d:Data {name: 'normal_stress'}) WHERE d.mean > 30 RETURN d")
#+end_src

#+RESULTS:
:  d                                                                                                                                                            
: --------------------------------------------------------------------------------------------------------------------------------------------------------------
:  (_5:Data {`25%`: 24.30745, `50%`: 33.5288, `75%`: 50.4053, count: 1407.0, max: 99.5891, mean: 38.06454, min: 1.99448, name: 'normal_stress', std: 19.35634})
* Modify project creation to support file path from response JSON

#+begin_src jupyter-python
def _create_or_return_node(node_type, node_key, graphdb, **node_props):
    matching = graphdb.nodes.match(node_type, **node_props)
    if len(matching) == 0:
        node = Node(node_type, **node_props)
        graphdb.create(node)
    else:
        node = list(matching)[0]
    return node


def _create_or_return_rel(node1, node2, r_type, graphdb):
    matching = graphdb.relationships.match(nodes=(node1, node2), r_type=r_type)
    if len(matching) == 0:
        rel = Relationship(node1, r_type, node2)
        graphdb.create(rel)
    else:
        rel = list(matching)[0]
    return rel


def create_project_graph(project_info, project_path, graphdb):
    project_id = project_info.get("project_id", None)
    if project_id is None:
        project_id = project_info.get("projectId", None)
    if project_id is None:
        raise KeyError("project_id or projectId not found in project info.")

    project_props = {"name": project_id}
    project_node = _create_or_return_node(
        "Project", {"name": project_id}, graphdb, **project_props
    )

    pi = project_info["project"]["value"]["pi"]
    pi_props = {"name": pi}
    pi_node = _create_or_return_node(
        "Author", {"name": pi}, graphdb, **pi_props
    )
    pi_project_rel = Relationship(pi_node, "SUPERVISES", project_node)
    graphdb.create(pi_project_rel)

    for nh in project_info["project"]["value"].get("nhTypes", []):
        nh_props = {"name": nh}
        nh_node = _create_or_return_node(
            "Hazard", nh_props, graphdb, **nh_props
        )
        _create_or_return_rel(project_node, nh_node, "NATURAL_HAZARD", graphdb)

    project_type = _create_or_return_node(
        "PROJECT_TYPE",
        {"name": "experimental"},
        graphdb,
        **{"name": "experimental"},
    )
    _create_or_return_rel(project_node, project_type, "TYPE", graphdb)
    for exp in tqdm(
        project_info["experimentsList"], desc="Experiment", leave=False
    ):
        experiment_props = {
            "uuid": exp["uuid"],
            "title": exp["value"]["title"],
        }
        exp_node = _create_or_return_node(
            "Experiment", {"uuid": exp["uuid"]}, graphdb, **experiment_props
        )
        _ = _create_or_return_rel(
            project_node, exp_node, "HAS_EXPERIMENT", graphdb
        )

    for model in tqdm(project_info["modelConfigs"], desc="Model", leave=False):
        model_props = {
            "uuid": model["uuid"],
            "title": model["value"]["title"],
        }
        model_node = _create_or_return_node(
            "Model", {"uuid": model["uuid"]}, graphdb, **model_props
        )
        model_exps = model["value"]["experiments"]
        for exp_uuid in model_exps:
            exp_matching = graphdb.nodes.match("Experiment", uuid=exp_uuid)
            if len(exp_matching) > 0:
                exp_node = list(exp_matching)[0]
                _ = _create_or_return_rel(
                    exp_node, model_node, "HAS_MODEL", graphdb
                )

    for event in tqdm(project_info["eventsList"], desc="Events", leave=False):
        event_props = {
            "title": event["value"]["title"],
        }
        event_node = _create_or_return_node(
            "Event", {"title": event_props["title"]}, graphdb, **event_props
        )
        event_exps = event["value"]["experiments"]
        for exp_uuid in event_exps:
            exp_matching = graphdb.nodes.match("Experiment", uuid=exp_uuid)
            if len(exp_matching) > 0:
                exp_node = list(exp_matching)[0]
                _ = _create_or_return_rel(
                    exp_node, event_node, "HAS_EVENT", graphdb
                )

        event_models = event["value"]["modelConfigs"]
        for model_uuid in event_models:
            model_matching = graphdb.nodes.match("Model", uuid=model_uuid)
            if len(model_matching) > 0:
                model_node = list(model_matching)[0]
                event_model_rel = Relationship(
                    model_node, "HAS_EVENT", event_node
                )
                graphdb.create(event_model_rel)
                _create_or_return_rel(
                    model_node, event_node, "HAS_EVENT", graphdb
                )

        for fdict in event["fileObjs"]:
            f = fdict["path"][1:]
            # extension = os.path.splitext(f)[1].lower()
            # if extension not in {".txt", ".csv"}:
            #     continue
            filepath = os.path.join(project_path, project_id, f)
            file_node = _create_or_return_node(
                "File",
                {"filepath": filepath},
                graphdb,
                **{"filepath": filepath},
            )
            _ = _create_or_return_rel(
                event_node, file_node, "HAS_FILE", graphdb
            )
            _, summary = summarize_csv(filepath)
            for key, props in summary.items():
                if not isinstance(props, dict):
                    props = {key: props}
                col_props = {"name": key, **props}
                column_node = _create_or_return_node(
                    "Data", {"name": key}, graphdb, **col_props
                )
                _ = _create_or_return_rel(
                    project_node, column_node, "RECORDS", graphdb
                )
                _ = _create_or_return_rel(
                    event_node, column_node, "RECORDS", graphdb
                )
                _ = _create_or_return_rel(
                    file_node, column_node, "RECORDS", graphdb
                )
    return
#+end_src
